{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__                           import print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot                  as plt\n",
    "import numpy                              as np\n",
    "import random                             as rd\n",
    "import tables                             as tb\n",
    "import tensorflow                         as tf\n",
    "\n",
    "from glob                                 import glob\n",
    "from matplotlib.patches                   import Ellipse\n",
    "\n",
    "\n",
    "# Keras imports\n",
    "import keras.backend.tensorflow_backend   as   K\n",
    "\n",
    "from keras                                import regularizers\n",
    "from keras                                import callbacks\n",
    "\n",
    "from keras.models                         import Model, load_model\n",
    "\n",
    "from keras.layers                         import Input, Dense, MaxPooling3D, AveragePooling3D\n",
    "from keras.layers                         import Conv3D, Conv2D, AveragePooling2D, Activation, Dropout, ZeroPadding3D\n",
    "from keras.layers                         import Add\n",
    "from keras.layers                         import Flatten, BatchNormalization\n",
    "\n",
    "from keras.layers.normalization           import BatchNormalization\n",
    "from keras.layers.convolutional           import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core                    import Flatten\n",
    "\n",
    "from keras.optimizers                     import SGD, Adam, Nadam, Adadelta \n",
    "\n",
    "from keras.regularizers                   import l2, l1\n",
    "from keras.initializers                   import RandomNormal\n",
    "\n",
    "from keras.utils.layer_utils              import print_summary\n",
    "\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a combined file from several .npz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Saving combined file\n"
     ]
    }
   ],
   "source": [
    "npz_dir = \"/home/amenkara/PETALO_dnns/dnn/\"\n",
    "\n",
    "# input/output files\n",
    "\n",
    "files = glob(npz_dir + '*.npz')\n",
    "\n",
    "files = sorted(files)\n",
    "print(files)\n",
    "\n",
    "out_file = \"combined_file_15_january.npz\"\n",
    "\n",
    "X_    = []\n",
    "Compt_= []\n",
    "Ev_   = []\n",
    "\n",
    "\n",
    "for f in files:\n",
    "\n",
    "    fn = np.load(f)\n",
    "    \n",
    "    if(len(X_) == 0):\n",
    "        X_     = fn['images']\n",
    "        Compt_ =  fn['labels']\n",
    "        Ev_    =  fn['event_number']\n",
    "    else:\n",
    "        X_     = np.concatenate((X_,     fn['images'])      )\n",
    "        Compt_ = np.concatenate((Compt_, fn['labels'])      ) \n",
    "        Ev_    = np.concatenate((Ev_,    fn['event_number']))\n",
    "    \n",
    "print(\"Saving combined file\")\n",
    "np.savez(out_file, X_=X_, Compt_=Compt_, Ev_=Ev_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = \"training_set_test.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(out_file)\n",
    "X_, Compt_, Ev_ = data['images'],data['labels'], data['event_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_compton_events_labels        =  []\n",
    "all_compton_events_images        =  []\n",
    "all_compton_events_id            =  []\n",
    "\n",
    "all_photoelectric_events_labels  =  []\n",
    "all_photoelectric_events_images  =  []\n",
    "all_photoelectric_events_id      =  []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the events into photoelectric and Compton.\n",
    "\n",
    "for i,event in enumerate(Ev_):\n",
    "    if Compt_[i]== 0:\n",
    "        all_photoelectric_events_labels.append(Compt_[i])\n",
    "        all_photoelectric_events_images.append(X_[i])\n",
    "        all_photoelectric_events_id.append(event)\n",
    "    if Compt_[i]==1:\n",
    "        all_compton_events_labels.append(Compt_[i])\n",
    "        all_compton_events_images.append(X_[i])\n",
    "        all_compton_events_id.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then cut the longest in order it to be of the same lenght.\n",
    "cut_compton_events_labels =  all_compton_events_labels[: len(all_photoelectric_events_id)]\n",
    "cut_compton_events_images =  all_compton_events_images[: len(all_photoelectric_events_id)]\n",
    "cut_compton_events_id     =  all_compton_events_id[: len(all_photoelectric_events_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train     = []\n",
    "Compt_train = []\n",
    "Ev_train    = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list     = all_photoelectric_events_images + cut_compton_events_images\n",
    "Compt_train_list = all_photoelectric_events_labels + cut_compton_events_labels\n",
    "Ev_train_list    = all_photoelectric_events_id     + cut_compton_events_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_= np.array(X_train_list)\n",
    "Compt_= np.array(Compt_train_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cut = int(0.8*len(X_))\n",
    "val_cut   = int(0.9*len(X_))\n",
    "\n",
    "X_train      = X_[:train_cut]\n",
    "X_val        = X_[train_cut:val_cut] \n",
    "X_test       = X_[val_cut:]\n",
    "\n",
    "Compt_train  =  Compt_[:train_cut]\n",
    "Compt_val    =  Compt_[train_cut:val_cut]\n",
    "Compt_test   =  Compt_[val_cut:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_CNN(inputs):\n",
    "    initial = \"random_uniform\"\n",
    "    #el 32 implica que hay 32 filtros de 5x5\n",
    "    cinputs = Conv2D(96, (5, 5), padding='same', strides=(2, 2), activation='relu', kernel_initializer=initial, kernel_regularizer=regularizers.l1(0.01),  activity_regularizer=regularizers.l1(0.01))(inputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', kernel_initializer=initial, kernel_regularizer=regularizers.l1(0.01),  activity_regularizer=regularizers.l1(0.01))(cinputs)\n",
    "    cinputs = Dropout(.2)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    cinputs = Conv2D(384, (2, 2), padding='same', strides=(1, 1), activation='relu', kernel_initializer=initial, kernel_regularizer=regularizers.l1(0.01), activity_regularizer=regularizers.l1(0.01))(cinputs)\n",
    "    cinputs = Dropout(.2)(cinputs)\n",
    "    cinputs = BatchNormalization(epsilon=1e-05, axis=3, momentum=0.99, weights=None, beta_initializer='zero', gamma_initializer='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "    cinputs = AveragePooling2D(pool_size=(2, 2), data_format=None, padding=\"same\", strides=(2, 2))(cinputs)\n",
    "    \n",
    "    f1 = Flatten()(cinputs)\n",
    "    f1 = Dense(units=64, activation='relu', kernel_initializer=initial)(f1)\n",
    "    #f1 = Dropout(.2)(f1)\n",
    "\n",
    "    inc_output = Dense(units=1, activation='sigmoid', kernel_initializer=initial)(f1)\n",
    "    \n",
    "    model = Model(inputs, inc_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.001), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(inputs):\n",
    "    cinputs = Conv2D(96, (5, 5))(inputs)\n",
    "    cinputs = Dense(32)(cinputs)\n",
    "    internal = Dense(32)(cinputs)\n",
    "    internal = Flatten()(internal)\n",
    "    output = Dense(1, activation='sigmoid')(internal)\n",
    "    \n",
    "    model = Model(inputs, output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim     = 30\n",
    "\n",
    "input_  = Input((dim,dim,1))\n",
    "model   = model_test(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30, 30, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 96)        2496      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 26, 26, 32)        3104      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 26, 26, 32)        1056      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 21633     \n",
      "=================================================================\n",
      "Total params: 28,289\n",
      "Trainable params: 28,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tboard = callbacks.TensorBoard(log_dir='./logs/detele_me', histogram_freq=0, write_graph=True, write_images=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 528 samples, validate on 66 samples\n",
      "Epoch 1/1\n",
      "528/528 [==============================] - 3s 6ms/step - loss: 0.6850 - val_loss: 0.7472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f97437d6e10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Compt_train, batch_size=18, epochs=1, shuffle=True, callbacks=[tboard],verbose=1, validation_data=(X_val, Compt_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_conv2d_16_january_last_layer_activation_is_relu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('model_conv2d_16_january.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test, batch_size=18, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cut is the accuracy. \n",
    "def plot_result(cut):\n",
    "elements = []\n",
    "for element in prediction:\n",
    "    if element > cut:\n",
    "        elements.append(0)\n",
    "    else:\n",
    "        elements.append(1)\n",
    "        \n",
    "    aciertos = 0\n",
    "    total = len(elements)\n",
    "    for i in range(total):\n",
    "        if elements[i]== Compt_test[i]:\n",
    "            aciertos += 1\n",
    "        \n",
    "    print(aciertos)    \n",
    "    return aciertos/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = []\n",
    "for i in range(500, 1000):\n",
    "    matrix.append([i/1000., plot_result(i/1000.)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(matrix)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.transpose(matrix)[0][:], np.transpose(matrix)[1][:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
